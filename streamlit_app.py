# streamlit_app.py

import streamlit as st
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import gc
import time
import os
from datetime import datetime
import logging

# Configuration des logs
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Configuration de la page
st.set_page_config(
    page_title="ü§ñ ChatBot IA - Deep Learning",
    page_icon="ü§ñ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# CSS personnalis√© moderne et √©pur√©
st.markdown("""
<style>
    /* Import Google Fonts */
    @import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap');
    
    /* Variables CSS */
    :root {
        --primary-color: #667eea;
        --secondary-color: #764ba2;
        --background-gradient: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        --border-radius: 12px;
        --shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
    }
    
    /* Header principal */
    .main-header {
        background: var(--background-gradient);
        padding: 2rem;
        border-radius: var(--border-radius);
        color: white;
        text-align: center;
        margin-bottom: 2rem;
        box-shadow: var(--shadow);
    }
    
    .main-header h1 {
        font-family: 'Inter', sans-serif;
        font-weight: 700;
        margin: 0;
        font-size: 2.2rem;
    }
    
    .main-header p {
        font-family: 'Inter', sans-serif;
        font-weight: 300;
        margin: 0.5rem 0 0 0;
        opacity: 0.9;
        font-size: 1rem;
    }
    
    /* Sidebar styling */
    .sidebar-content {
        font-family: 'Inter', sans-serif;
    }
    
    .stats-card {
        background: white;
        padding: 1rem;
        border-radius: var(--border-radius);
        border: 1px solid #e0e0e0;
        margin-bottom: 1rem;
        box-shadow: var(--shadow);
        text-align: center;
    }
    
    .stats-number {
        font-size: 1.8rem;
        font-weight: 700;
        color: var(--primary-color);
    }
    
    .stats-label {
        font-size: 0.85rem;
        color: #666;
        margin-top: 0.25rem;
    }
    
    /* Status indicators */
    .status-online {
        color: #10b981;
        font-weight: 600;
    }
    
    .status-loading {
        color: #f59e0b;
        font-weight: 600;
    }
    
    .status-error {
        color: #ef4444;
        font-weight: 600;
    }
    
    /* Button styling */
    .stButton > button {
        background: var(--background-gradient);
        color: white;
        border: none;
        border-radius: var(--border-radius);
        font-family: 'Inter', sans-serif;
        font-weight: 500;
        transition: all 0.3s ease;
    }
    
    .stButton > button:hover {
        transform: translateY(-1px);
        box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
    }
    
    /* Welcome message */
    .welcome-card {
        text-align: center;
        padding: 3rem;
        background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%);
        border-radius: 15px;
        margin: 2rem 0;
    }
    
    .welcome-card h2 {
        color: #667eea;
        margin-bottom: 1rem;
        font-family: 'Inter', sans-serif;
    }
    
    .welcome-card p {
        font-size: 1.1rem;
        color: #666;
        margin-bottom: 1.5rem;
        font-family: 'Inter', sans-serif;
    }
    
    /* Performance metrics */
    .perf-metric {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        padding: 0.5rem 1rem;
        border-radius: 20px;
        font-size: 0.8rem;
        margin: 0.2rem;
        display: inline-block;
    }
    
    /* Responsive design */
    @media (max-width: 768px) {
        .main-header h1 { font-size: 1.8rem; }
        .main-header p { font-size: 0.9rem; }
        .welcome-card { padding: 2rem 1rem; }
    }
</style>
""", unsafe_allow_html=True)

@st.cache_resource(show_spinner=False)
def load_model():
    """Chargement du mod√®le avec mise en cache optimis√©e et fallback"""
    start_time = time.time()
    
    try:
        # Priorit√© aux mod√®les locaux optimis√©s
        model_paths = [
            "./chatbot_model_optimized",
            "./chatbot_model",
            "microsoft/DialoGPT-medium"  # Fallback HuggingFace
        ]
        
        for i, model_path in enumerate(model_paths):
            try:
                logger.info(f"Tentative de chargement depuis: {model_path}")
                
                # Configuration selon le type de mod√®le
                is_local = not model_path.startswith("microsoft/")
                load_config = {
                    "local_files_only": is_local,
                    "trust_remote_code": False
                }
                
                # Chargement du tokenizer
                with st.spinner(f"üîÑ Chargement du tokenizer ({i+1}/{len(model_paths)})..."):
                    tokenizer = AutoTokenizer.from_pretrained(
                        model_path,
                        **load_config
                    )
                
                # Chargement du mod√®le
                with st.spinner(f"üîÑ Chargement du mod√®le ({i+1}/{len(model_paths)})..."):
                    model = AutoModelForCausalLM.from_pretrained(
                        model_path,
                        torch_dtype=torch.float32,
                        device_map="cpu",
                        **load_config
                    )
                
                # Configuration du mod√®le
                model.eval()
                
                # Configuration du pad_token
                if tokenizer.pad_token is None:
                    tokenizer.pad_token = tokenizer.eos_token
                    tokenizer.pad_token_id = tokenizer.eos_token_id
                
                load_time = time.time() - start_time
                model_size = sum(p.numel() for p in model.parameters()) / 1e6  # Millions de param√®tres
                
                logger.info(f"Mod√®le charg√© avec succ√®s en {load_time:.2f}s")
                
                # Message de succ√®s avec d√©tails
                source = "Local" if is_local else "HuggingFace Hub"
                st.success(f"‚úÖ Mod√®le charg√© depuis {source} en {load_time:.1f}s")
                
                return tokenizer, model, True, {
                    "source": source,
                    "path": model_path,
                    "load_time": load_time,
                    "model_size": model_size,
                    "parameters": f"{model_size:.0f}M"
                }
                
            except Exception as e:
                logger.warning(f"√âchec du chargement depuis {model_path}: {e}")
                if i < len(model_paths) - 1:
                    st.warning(f"‚ö†Ô∏è {model_path} non disponible, tentative suivante...")
                continue
        
        raise Exception("Tous les chemins de mod√®le ont √©chou√©")
        
    except Exception as e:
        logger.error(f"Erreur critique lors du chargement: {e}")
        st.error(f"‚ùå Impossible de charger le mod√®le: {e}")
        return None, None, False, {}

@st.cache_data(ttl=300, show_spinner=False)  # Cache 5 minutes
def get_model_info(model_config):
    """Informations d√©taill√©es sur le mod√®le"""
    return {
        "architecture": "DialoGPT-medium",
        "parameters": model_config.get("parameters", "345M"),
        "layers": 24,
        "attention_heads": 16,
        "vocabulary_size": "50,257",
        "context_length": 1024
    }

def generate_response(user_input, tokenizer, model, chat_history_ids=None, max_attempts=3):
    """G√©n√©ration de r√©ponse optimis√©e avec retry logic"""
    
    for attempt in range(max_attempts):
        try:
            # Nettoyage de l'entr√©e
            user_input = user_input.strip()
            if not user_input:
                return "Pouvez-vous r√©p√©ter votre message ?", chat_history_ids
            
            # Encodage de l'entr√©e utilisateur
            encoded = tokenizer(
                user_input + tokenizer.eos_token,
                return_tensors='pt',
                padding=True,
                truncation=True,
                max_length=512
            )
            
            new_user_input_ids = encoded['input_ids']
            
            # Gestion intelligente de l'historique
            if chat_history_ids is not None:
                # Limiter la taille de l'historique pour √©viter les d√©bordements
                if chat_history_ids.shape[-1] > 700:
                    # Garder seulement les 500 derniers tokens
                    chat_history_ids = chat_history_ids[:, -500:]
                
                bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1)
            else:
                bot_input_ids = new_user_input_ids
            
            # G√©n√©ration avec param√®tres optimis√©s
            generation_config = {
                "max_length": min(bot_input_ids.shape[-1] + 100, 1000),
                "num_return_sequences": 1,
                "temperature": 0.8,
                "pad_token_id": tokenizer.pad_token_id,
                "eos_token_id": tokenizer.eos_token_id,
                "do_sample": True,
                "top_k": 50,
                "top_p": 0.95,
                "repetition_penalty": 1.15,
                "no_repeat_ngram_size": 3,
                "early_stopping": True
            }
            
            with torch.no_grad():
                chat_history_ids = model.generate(
                    bot_input_ids,
                    **generation_config
                )
            
            # Extraction de la r√©ponse
            response = tokenizer.decode(
                chat_history_ids[:, bot_input_ids.shape[-1]:][0], 
                skip_special_tokens=True
            ).strip()
            
            # Post-traitement de la r√©ponse
            if not response or len(response) < 2:
                response = "Je ne suis pas s√ªr de comprendre. Pouvez-vous reformuler ?"
            
            # Nettoyage des r√©p√©titions en fin de phrase
            sentences = response.split('.')
            if len(sentences) > 1 and sentences[-2] == sentences[-1]:
                response = '.'.join(sentences[:-1]) + '.'
            
            # Nettoyage m√©moire
            del bot_input_ids, encoded
            gc.collect()
            
            return response, chat_history_ids
            
        except Exception as e:
            logger.warning(f"Tentative {attempt + 1} √©chou√©e: {e}")
            if attempt == max_attempts - 1:
                return "D√©sol√©, je rencontre des difficult√©s techniques. Pouvez-vous r√©essayer ?", chat_history_ids
            time.sleep(0.5)  # Pause courte avant retry
    
    return "Erreur technique persistante.", chat_history_ids

def response_generator(response_text, speed=0.03):
    """G√©n√©rateur pour l'effet de streaming am√©lior√©"""
    if not response_text:
        yield "..."
        return
    
    words = response_text.split()
    for i, word in enumerate(words):
        yield word + " "
        # Pause plus longue apr√®s la ponctuation
        if word.endswith(('.', '!', '?', ':')):
            time.sleep(speed * 3)
        else:
            time.sleep(speed)

def reset_conversation():
    """Reset complet de la conversation"""
    st.session_state.messages = []
    st.session_state.chat_history_ids = None
    st.session_state.conversation_count = 0
    st.session_state.total_tokens_generated = 0
    gc.collect()  # Nettoyage m√©moire

def main():
    # Header principal avec animation
    st.markdown("""
    <div class="main-header">
        <h1>ü§ñ ChatBot IA - Deep Learning</h1>
        <p>Propuls√© par DialoGPT-medium ‚Ä¢ Transformers ‚Ä¢ Dataset DailyDialog</p>
    </div>
    """, unsafe_allow_html=True)
    
    # Initialisation de l'√©tat de session
    if "messages" not in st.session_state:
        st.session_state.messages = []
    if "chat_history_ids" not in st.session_state:
        st.session_state.chat_history_ids = None
    if "conversation_count" not in st.session_state:
        st.session_state.conversation_count = 0
    if "total_tokens_generated" not in st.session_state:
        st.session_state.total_tokens_generated = 0
    if "model_loaded" not in st.session_state:
        st.session_state.model_loaded = False
    
    # Chargement du mod√®le avec gestion d'√©tat
    if not st.session_state.model_loaded:
        with st.spinner("üöÄ Initialisation du syst√®me IA..."):
            tokenizer, model, model_loaded, model_config = load_model()
            
            if model_loaded:
                st.session_state.tokenizer = tokenizer
                st.session_state.model = model
                st.session_state.model_config = model_config
                st.session_state.model_loaded = True
                st.session_state.model_info = get_model_info(model_config)
    
    if not st.session_state.model_loaded:
        st.error("‚ùå Impossible de charger le mod√®le. V√©rifiez la configuration.")
        st.info("üí° Le mod√®le sera t√©l√©charg√© automatiquement depuis HuggingFace si les fichiers locaux ne sont pas disponibles.")
        return
    
    # Sidebar avec statistiques et contr√¥les
    with st.sidebar:
        st.markdown('<div class="sidebar-content">', unsafe_allow_html=True)
        
        # Status du syst√®me
        st.markdown("## üîå Statut Syst√®me")
        col1, col2 = st.columns([1, 2])
        with col1:
            st.markdown("üü¢")
        with col2:
            st.markdown('<span class="status-online">En ligne</span>', unsafe_allow_html=True)
        
        # Informations du mod√®le
        if "model_config" in st.session_state:
            config = st.session_state.model_config
            st.markdown(f"""
            **üè∑Ô∏è Source:** {config['source']}  
            **‚ö° Chargement:** {config['load_time']:.1f}s  
            **üß† Param√®tres:** {config['parameters']}
            """)
        
        st.markdown("## üìä Statistiques en Temps R√©el")
        
        # Cartes de statistiques am√©lior√©es
        col1, col2 = st.columns(2)
        with col1:
            st.markdown(f"""
            <div class="stats-card">
                <div class="stats-number">{len(st.session_state.messages)}</div>
                <div class="stats-label">Messages</div>
            </div>
            """, unsafe_allow_html=True)
        
        with col2:
            st.markdown(f"""
            <div class="stats-card">
                <div class="stats-number">{st.session_state.conversation_count}</div>
                <div class="stats-label">Tours</div>
            </div>
            """, unsafe_allow_html=True)
        
        # M√©triques de performance
        if hasattr(st.session_state, 'total_tokens_generated'):
            st.markdown(f"""
            <div class="perf-metric">Tokens g√©n√©r√©s: {st.session_state.total_tokens_generated}</div>
            """, unsafe_allow_html=True)
        
        st.markdown("## ‚öôÔ∏è Contr√¥les")
        
        # Boutons de contr√¥le
        if st.button("üóëÔ∏è Nouvelle Conversation", use_container_width=True):
            reset_conversation()
            st.success("üí¨ Conversation r√©initialis√©e!")
            time.sleep(1)
            st.rerun()
        
        if st.button("üîÑ Red√©marrer IA", use_container_width=True):
            st.session_state.model_loaded = False
            st.cache_resource.clear()
            st.success("ü§ñ IA red√©marr√©e!")
            time.sleep(1)
            st.rerun()
        
        # Param√®tres avanc√©s
        with st.expander("üîß Param√®tres Avanc√©s"):
            st.markdown("""
            **R√©initialisation automatique:** Apr√®s 10 √©changes  
            **Longueur max r√©ponse:** 100 tokens  
            **Temperature:** 0.8 (cr√©ativit√©)  
            **R√©p√©tition:** Penalty 1.15
            """)
        
        st.markdown("## ‚ÑπÔ∏è Informations Techniques")
        if "model_info" in st.session_state:
            info = st.session_state.model_info
            st.markdown(f"""
            **üèóÔ∏è Architecture:** {info['architecture']}  
            **üìä Param√®tres:** {info['parameters']}  
            **üîß Couches:** {info['layers']}  
            **üëÅÔ∏è T√™tes attention:** {info['attention_heads']}  
            **üìö Vocabulaire:** {info['vocabulary_size']}  
            **üíæ Contexte:** {info['context_length']} tokens
            """)
        
        # Exemples de questions avec cat√©gories
        st.markdown("## üí° Suggestions de Conversation")
        
        example_categories = {
            "üéØ Questions g√©n√©rales": [
                "Hello, how are you today?",
                "What's your favorite hobby?",
                "Tell me about yourself"
            ],
            "üé≠ Divertissement": [
                "Tell me a joke",
                "What's a fun fact?",
                "Share an interesting story"
            ],
            "ü§î Conversations profondes": [
                "What do you think about AI?",
                "How was your day?",
                "What motivates you?"
            ]
        }
        
        for category, questions in example_categories.items():
            with st.expander(category):
                for question in questions:
                    if st.button(f"üí¨ {question}", key=f"example_{hash(question)}", use_container_width=True):
                        # Ajouter √† l'historique et traiter
                        st.session_state.messages.append({"role": "user", "content": question})
                        
                        # G√©n√©rer la r√©ponse
                        with st.spinner("ü§î G√©n√©ration de la r√©ponse..."):
                            bot_response, new_chat_history_ids = generate_response(
                                question, 
                                st.session_state.tokenizer, 
                                st.session_state.model, 
                                st.session_state.chat_history_ids
                            )
                        
                        st.session_state.messages.append({"role": "assistant", "content": bot_response})
                        st.session_state.chat_history_ids = new_chat_history_ids
                        st.session_state.conversation_count += 1
                        st.session_state.total_tokens_generated += len(bot_response.split())
                        
                        # Reset automatique apr√®s 10 √©changes
                        if st.session_state.conversation_count >= 10:
                            st.session_state.chat_history_ids = None
                            st.session_state.conversation_count = 0
                        
                        st.rerun()
        
        st.markdown('</div>', unsafe_allow_html=True)
    
    # Affichage de l'historique avec les √©l√©ments de chat natifs
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
    
    # Input de chat natif Streamlit
    if prompt := st.chat_input("üí≠ Tapez votre message ici..."):
        # Validation de l'entr√©e
        if len(prompt.strip()) == 0:
            st.warning("‚ö†Ô∏è Veuillez saisir un message non vide.")
            return
        
        if len(prompt) > 500:
            st.warning("‚ö†Ô∏è Message trop long (max 500 caract√®res).")
            return
        
        # Affichage du message utilisateur
        st.chat_message("user").markdown(prompt)
        st.session_state.messages.append({"role": "user", "content": prompt})
        
        # G√©n√©ration et affichage de la r√©ponse
        with st.chat_message("assistant"):
            start_time = time.time()
            
            with st.spinner("ü§î R√©flexion en cours..."):
                bot_response, new_chat_history_ids = generate_response(
                    prompt, 
                    st.session_state.tokenizer, 
                    st.session_state.model, 
                    st.session_state.chat_history_ids
                )
            
            generation_time = time.time() - start_time
            
            # Affichage avec streaming
            response = st.write_stream(response_generator(bot_response))
            
            # M√©trique de performance (optionnel, masqu√© par d√©faut)
            if st.button("üìà D√©tails g√©n√©ration", key="perf_details"):
                st.caption(f"‚è±Ô∏è G√©n√©r√© en {generation_time:.2f}s ‚Ä¢ {len(bot_response.split())} mots")
        
        # Mise √† jour de l'√©tat
        st.session_state.messages.append({"role": "assistant", "content": response})
        st.session_state.chat_history_ids = new_chat_history_ids
        st.session_state.conversation_count += 1
        st.session_state.total_tokens_generated += len(response.split())
        
        # Reset automatique apr√®s 10 √©changes avec notification
        if st.session_state.conversation_count >= 10:
            st.session_state.chat_history_ids = None
            st.session_state.conversation_count = 0
            st.info("üîÑ Contexte automatiquement r√©initialis√© pour optimiser les performances.")
    
    # Message de bienvenue si pas d'historique
    if not st.session_state.messages:
        st.markdown("""
        <div class="welcome-card">
            <h2>üëã Bienvenue dans votre ChatBot IA !</h2>
            <p>
                Je suis votre assistant IA bas√© sur DialoGPT-medium, entra√Æn√© sur des conversations naturelles.<br>
                Commencez une conversation en tapant un message ci-dessous !
            </p>
            <p style="font-size: 0.9rem; color: #888;">
                üí° Utilisez les suggestions dans la barre lat√©rale pour d√©couvrir mes capacit√©s
            </p>
        </div>
        """, unsafe_allow_html=True)
    
    # Footer avec informations techniques d√©taill√©es
    with st.expander("üîß Informations Techniques Compl√®tes"):
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.markdown("""
            **üèóÔ∏è Architecture Mod√®le**
            - Mod√®le: DialoGPT-medium
            - Param√®tres: 345M
            - Couches: 24 layers
            - T√™tes d'attention: 16
            - Embedding: 1024
            - Vocabulaire: 50,257 tokens
            """)
        
        with col2:
            st.markdown("""
            **‚öôÔ∏è Param√®tres G√©n√©ration**
            - Temperature: 0.8
            - Top-k: 50
            - Top-p: 0.95
            - Max length: 1000 tokens
            - Repetition penalty: 1.15
            - No repeat n-gram: 3
            """)
        
        with col3:
            st.markdown("""
            **üìä Dataset & Performance**
            - Source: DailyDialog
            - Dialogues: 13,118
            - Tours moyens: 7.9
            - Domaines: Conversations quotidiennes
            - Optimis√©: CPU Streamlit Cloud
            - Auto-reset: 10 √©changes
            """)
        
        # Informations syst√®me
        st.markdown("**üíª Informations Syst√®me**")
        system_info = {
            "PyTorch Version": torch.__version__,
            "Device": "CPU (Streamlit Cloud optimized)",
            "Memory Management": "Auto garbage collection",
            "Streaming": "Real-time word streaming",
            "Cache": "Model cached with Streamlit @cache_resource"
        }
        
        for key, value in system_info.items():
            st.text(f"{key}: {value}")

if __name__ == "__main__":
    main()
